{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "import lz4.frame\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "\n",
    "from time import perf_counter\n",
    "\n",
    "\n",
    "from utils import get_defect_list\n",
    "#import massPy as mp\n",
    "import massPy_dev.massPy as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CompressArchive:\n",
    "    def __init__(self, archive_dir, output_dir = None, dtype_out = 'float64', delete_archive_if_successful = False):\n",
    "\n",
    "        self.archive_dir = archive_dir\n",
    "        self.output_dir = archive_dir + '_compressed' if output_dir is None else output_dir\n",
    "\n",
    "        self.archive = mp.archive.loadarchive(archive_dir)\n",
    "        self.frame_list = self.__find_missing_frames()\n",
    "        self.failed_conversion_list = []\n",
    "\n",
    "        self.num_frames = len(self.frame_list)\n",
    "        self.LX = self.archive.LX\n",
    "        self.LY = self.archive.LY\n",
    "\n",
    "\n",
    "        self.dtype_out = dtype_out\n",
    "        self.delete_archive_if_successful = delete_archive_if_successful\n",
    "\n",
    "        self.time_to_open_json = np.nan\n",
    "        self.time_to_open_npz = np.nan\n",
    "        self.compression_ratio = np.nan\n",
    "        self.json_frame_size = np.nan\n",
    "        self.npz_frame_size = np.nan\n",
    "\n",
    "    def __calc_density(self, ff):\n",
    "        return np.sum(ff, axis=1)\n",
    "\n",
    "    def __calc_velocity(self, ff, density = None):\n",
    "        d = self.__calc_density(ff) if density is None else density\n",
    "        return np.asarray([ (ff.T[1] - ff.T[2] + ff.T[5] - ff.T[6] - ff.T[7] + ff.T[8]) / d,\n",
    "                            (ff.T[3] - ff.T[4] + ff.T[5] - ff.T[6] + ff.T[7] - ff.T[8]) / d\n",
    "                        ]).reshape(2, self.LX, self.LY)\n",
    "\n",
    "    def __estimate_size_reduction(self,):\n",
    "\n",
    "        input_file = os.path.join(self.archive_dir, f'frame{self.frame_list[0]}.json')\n",
    "        output_file = os.path.join(self.output_dir, f'frame{self.frame_list[0]}.npz')\n",
    "\n",
    "        if not os.path.exists(output_file):\n",
    "            print(f'File {output_file} does not exist')\n",
    "            return\n",
    "\n",
    "        # Get the sizes in bytes of the input and output archives\n",
    "        self.json_frame_size = os.path.getsize(input_file)\n",
    "        self.npz_frame_size = os.path.getsize(output_file)\n",
    "\n",
    "        # Calculate the compression ratio\n",
    "        self.compression_ratio = self.json_frame_size / self.npz_frame_size\n",
    "        return \n",
    "  \n",
    "    def __get_time_to_open_npz(self, frame_number = None):\n",
    "\n",
    "        if np.isnan(self.time_to_open_npz):\n",
    "            frame_number = self.frame_list[0] if frame_number is None else frame_number\n",
    "            npz_file_path = os.path.join(self.output_dir, f'frame{frame_number}.npz')\n",
    "            if not os.path.exists(npz_file_path):\n",
    "                print(f'File {npz_file_path} does not exist')\n",
    "                return\n",
    "            try:\n",
    "                t_start = perf_counter()\n",
    "                npz = np.load(npz_file_path, allow_pickle=True)\n",
    "                self.time_to_open_npz = perf_counter() - t_start\n",
    "            except:\n",
    "                print(f'Error opening file {npz_file_path}')\n",
    "        else:\n",
    "            pass\n",
    "        return\n",
    "\n",
    "    def __find_missing_frames(self):\n",
    "        dir_list = os.listdir(self.archive_dir)\n",
    "        frame_list = []\n",
    "\n",
    "        for item in dir_list:\n",
    "            if item.startswith(\"frame\"):\n",
    "                frame_num = int(item.split('.')[0].split('frame')[-1])\n",
    "                frame_list.append(frame_num)\n",
    "\n",
    "        if len(frame_list) == self.archive.num_frames:\n",
    "            return np.arange(self.archive.nstart, self.archive.nsteps + 1, self.archive.ninfo)\n",
    "        else:\n",
    "            frame_list.sort()\n",
    "            return frame_list\n",
    "        \n",
    "    def __unpack_json_dict(self, json_dict, exclude_keys=[], calc_velocities = False):\n",
    "\n",
    "        keys = list(json_dict['data'].keys())\n",
    "        arr_dict = {key: np.array(json_dict['data'][key]['value'],dtype=self.dtype_out) for key in keys if key not in exclude_keys}\n",
    "\n",
    "        if calc_velocities:\n",
    "            ff = np.array(json_dict['data']['ff']['value'],dtype='float64')\n",
    "            arr_dict['density'] = self.__calc_density(ff).astype(self.dtype_out)         \n",
    "            v = self.__calc_velocity(ff, arr_dict['density'])\n",
    "            arr_dict['vx'] = v[0].flatten().astype(self.dtype_out)\n",
    "            arr_dict['vy'] = v[1].flatten().astype(self.dtype_out)\n",
    "        return arr_dict\n",
    "\n",
    "    def __convert_json_to_npz(self, frame_number, overwrite_existing_npz_files = False, compress = True, exclude_keys=[], calc_velocities = False):\n",
    "\n",
    "        frame_input_path = os.path.join(self.archive_dir, f'frame{frame_number}.json')\n",
    "        frame_output_path = os.path.join(self.output_dir, f'frame{frame_number}.npz')\n",
    "\n",
    "        if os.path.exists(frame_output_path) and not overwrite_existing_npz_files:\n",
    "            print(f'File {frame_output_path} already exists. Skipping...')\n",
    "            return\n",
    "\n",
    "        if np.isnan(self.time_to_open_json):\n",
    "            t_start = perf_counter()\n",
    "            with open(frame_input_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            self.time_to_open_json = perf_counter() - t_start\n",
    "        else:\n",
    "            with open(frame_input_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "        arr_dict = self.__unpack_json_dict(data, exclude_keys=exclude_keys, calc_velocities = calc_velocities)\n",
    "        if compress:\n",
    "            np.savez_compressed(frame_output_path, **arr_dict)\n",
    "        else:\n",
    "            np.savez(frame_output_path, **arr_dict)\n",
    "        return\n",
    "\n",
    "    def convert_archive_to_npz(self, compress = True, exclude_keys=[], calc_velocities = False, overwrite_existing_npz_files = False, verbose = 1,):\n",
    "        # Create the output folder if it does not exist\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "\n",
    "        # copy parameters.json to output folder\n",
    "        parameters_path = os.path.join(self.archive_dir, 'parameters.json')\n",
    "        shutil.copy(parameters_path, self.output_dir)\n",
    "\n",
    "        if verbose > 0:\n",
    "            start = perf_counter()\n",
    "\n",
    "        for i, frame in enumerate(self.frame_list):\n",
    "            try:\n",
    "                if verbose == 2:\n",
    "                    start_frame = perf_counter()\n",
    "                self.__convert_json_to_npz(frame, overwrite_existing_npz_files = overwrite_existing_npz_files,\\\n",
    "                                            compress = compress, exclude_keys = exclude_keys, calc_velocities = calc_velocities)\n",
    "                if verbose == 2:\n",
    "                    print(f'Frame {frame} processed in {perf_counter() - start_frame:.2f} seconds')\n",
    "            except:\n",
    "                print(f'Error processing frame {frame}. Skipping...')\n",
    "                self.failed_conversion_list.append(frame)\n",
    "\n",
    "        if verbose > 0:\n",
    "            print(f'Archive processed in {perf_counter() - start:.2f} seconds with {len(self.failed_conversion_list)} failed conversions')\n",
    "            if len(self.failed_conversion_list) > 0:\n",
    "                print(f'Frames for which conversion to npz failed: {self.failed_conversion_list}')\n",
    "            self.print_conversion_info()\n",
    "\n",
    "        if self.delete_archive_if_successful and len(self.failed_conversion_list) == 0:\n",
    "            shutil.rmtree(self.archive_dir)\n",
    "            print(f'Archive {self.archive_dir} deleted')\n",
    "        return\n",
    "\n",
    "    def print_conversion_info(self):\n",
    "\n",
    "        self.__estimate_size_reduction()\n",
    "        self.__get_time_to_open_npz()\n",
    "    \n",
    "        print(f'\\nEstimated size reduction for entire archive ({self.num_frames} frames):')\n",
    "        print(f'Uncompressed archive size: {self.json_frame_size / 1024 ** 2 * self.num_frames:.2f} MB')\n",
    "        print(f'Compressed archive size: {self.npz_frame_size / 1024 ** 2 * self.num_frames:.2f} MB')\n",
    "        print(f'Compression ratio: {self.compression_ratio:.2f}x\\n')\n",
    "\n",
    "        print(f'Time to open json frame: {self.time_to_open_json:.2f} seconds')\n",
    "        print(f'Time to open npz frame: {self.time_to_open_npz:.2f} seconds')\n",
    "        print(f'Speedup: {self.time_to_open_json / self.time_to_open_npz:.2f}x')\n",
    "        return\n",
    "\n",
    "    def get_conversion_info(self):\n",
    "\n",
    "        self.__estimate_size_reduction()\n",
    "        self.__get_time_to_open_npz()\n",
    "\n",
    "        return {'archive_dir': self.archive_dir,\n",
    "                'output_dir': self.output_dir,\n",
    "                'num_frames': self.num_frames,\n",
    "                'frame_list': self.frame_list,\n",
    "                'json_frame_size': self.json_frame_size,\n",
    "                'npz_frame_size': self.npz_frame_size,\n",
    "                'failed_conversion_list': self.failed_conversion_list,\n",
    "                'time_to_open_json_frame': self.time_to_open_json,\n",
    "                'time_to_open_npz_frame': self.time_to_open_npz,\n",
    "                'compression_ratio': self.compression_ratio,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_file(input_file, output_file):\n",
    "    # Open the input file in binary mode for reading\n",
    "    with open(input_file, 'rb') as f_in:\n",
    "        # Read the data from the input file\n",
    "        data = f_in.read()\n",
    "\n",
    "        # Compress the data using lz4 compression\n",
    "        compressed_data = lz4.frame.compress(data)\n",
    "\n",
    "        # Write the compressed data to the output file\n",
    "        with open(output_file, 'wb') as f_out:\n",
    "            f_out.write(compressed_data)\n",
    "    return\n",
    "\n",
    "def print_size(input_file, output_file):\n",
    "    # Get the size of the input file\n",
    "    input_size = os.path.getsize(input_file)\n",
    "\n",
    "    # Get the size of the output file\n",
    "    output_size = os.path.getsize(output_file)\n",
    "\n",
    "    # Print the size of the input and output files\n",
    "    print(f'Input file size: {input_size / 1024 ** 2:.2f} MB')\n",
    "    print(f'Output file size: {output_size / 1024 ** 2:.2f} MB')\n",
    "\n",
    "    # Calculate the compression ratio\n",
    "    ratio = input_size / output_size\n",
    "    print(f'Compression ratio: {ratio:.2f}x')\n",
    "    return\n",
    "\n",
    "def estimate_size_reduction(input_file, output_file, Nframes = 1, verbose=True):\n",
    "    # Get the size of the input file\n",
    "    input_size = os.path.getsize(input_file)\n",
    "\n",
    "    # Get the size of the output file\n",
    "    output_size = os.path.getsize(output_file)\n",
    "\n",
    "    # Calculate the compression ratio\n",
    "    ratio = input_size / output_size\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Uncompressed archive size: {input_size / 1024 ** 2 * Nframes:.2f} MB')\n",
    "        print(f'Compressed archive size: {output_size / 1024 ** 2 * Nframes:.2f} MB')\n",
    "        print(f'Compression ratio: {ratio:.2f}x\\n')\n",
    "\n",
    "    return input_size * Nframes, output_size * Nframes, ratio\n",
    "\n",
    "def decompress_and_convert(input_file, out_format = 'json'):\n",
    "    # Open the input file in binary mode for reading\n",
    "    with open(input_file, 'rb') as f_in:\n",
    "        # Read the compressed data from the input file\n",
    "        compressed_data = f_in.read()\n",
    "\n",
    "        # Decompress the data using lz4 decompression\n",
    "        decompressed_data = lz4.frame.decompress(compressed_data)    \n",
    "\n",
    "        if out_format == 'json':\n",
    "            # Decode the bytes to string\n",
    "            decoded_data = decompressed_data.decode('utf-8')\n",
    "            json_data = json.loads(decoded_data)\n",
    "            return json_data       \n",
    "        elif out_format == 'npz':\n",
    "            # Decode the bytes to string and parse JSON\n",
    "            npz_data = npz_data = np.load(io.BytesIO(decompressed_data), allow_pickle=True)\n",
    "            return npz_data      \n",
    "        else:\n",
    "            print('Invalid output format. Please use \"json\" or \"npz\"')\n",
    "            return\n",
    "        \n",
    "def unpack_arrays(json_dict, dtype_out = 'float64', exclude_keys=[]):\n",
    "    keys = list(json_dict['data'].keys())\n",
    "    arr_dict = {}\n",
    "    arr_dict = {key: np.array(json_dict['data'][key]['value'],dtype=dtype_out) for key in keys if key not in exclude_keys}\n",
    "   # for key in keys:\n",
    "    #    arr_dict[key] = np.array(json_dict['data'][key]['value'],dtype=dtype_out)\n",
    "    return arr_dict\n",
    "\n",
    "def unpack_nematic_json_dict(json_dict, dtype_out = 'float64', exclude_keys=[], calc_velocities = False):\n",
    "    keys = list(json_dict['data'].keys())\n",
    "    arr_dict = {}\n",
    "    arr_dict = {key: np.array(json_dict['data'][key]['value'],dtype=dtype_out) for key in keys if key not in exclude_keys}\n",
    "\n",
    "    if calc_velocities:\n",
    "        ff = np.array(json_dict['data']['ff']['value'],dtype=dtype_out)\n",
    "        arr_dict['vx'], arr_dict['vy'] = mp.base_modules.flow.velocity(ff,)\n",
    "    return arr_dict\n",
    "\n",
    "def find_missing_frames(archive_path):\n",
    "\n",
    "    ar = mp.archive.loadarchive(dir)\n",
    "\n",
    "    dir_list = os.listdir(archive_path)\n",
    "    frame_list = []\n",
    "\n",
    "    for item in dir_list:\n",
    "        if item.startswith(\"frame\"):\n",
    "            frame_num = int(item.split('.')[0].split('frame')[-1])\n",
    "            frame_list.append(frame_num)\n",
    "\n",
    "    if len(frame_list) == ar.num_frames:\n",
    "        return np.arange(ar.nstart, ar.nsteps + 1, ar.ninfo)\n",
    "    else:\n",
    "        frame_list.sort()\n",
    "        return frame_list\n",
    "    \n",
    "def convert_json_to_npz(json_path, out_path, compress = True, dtype_out = 'float64', exclude_keys=[]):\n",
    "\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    arr_dict = unpack_arrays(data,  dtype_out = dtype_out, exclude_keys=exclude_keys)\n",
    "    if compress:\n",
    "        np.savez_compressed(out_path, **arr_dict)\n",
    "    else:\n",
    "        np.savez(out_path, **arr_dict)\n",
    "    return\n",
    "\n",
    "def create_npz_folder(archive_path, output_folder = None, check_for_missing_frames = False, compress = True, \\\n",
    "                      dtype_out= 'float32', exclude_keys=[], verbose = 1):\n",
    "    \"\"\"\n",
    "    verbose = 0: no output\n",
    "    verbose = 1: print time to process entire archive\n",
    "    verbose = 2: print time to process each frame\n",
    "    \"\"\"\n",
    "    # Create the output folder if it does not exist\n",
    "\n",
    "    output_folder = archive_path + '_npz' if output_folder is None else output_folder\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # copy parameters.json to output folder\n",
    "    parameters_path = os.path.join(archive_path, 'parameters.json')\n",
    "    shutil.copy(parameters_path, output_folder)\n",
    "\n",
    "    # Load the archive and get the list of frames\n",
    "    ar = mp.archive.loadarchive(archive_path)\n",
    "    frame_list = find_missing_frames(archive_path) if check_for_missing_frames else np.arange(ar.nstart, ar.nsteps + 1, ar.ninfo)\n",
    "\n",
    "    # initialize failed conversions list\n",
    "    failed_conversions = []\n",
    "\n",
    "    if verbose > 0:\n",
    "        start = perf_counter()\n",
    "\n",
    "    for i, frame in enumerate(frame_list):\n",
    "        frame_input_path = os.path.join(archive_path, f'frame{frame}.json')\n",
    "        frame_output_path = os.path.join(output_folder, f'frame{frame}.npz')\n",
    "        try:\n",
    "            if verbose == 2:\n",
    "                start_frame = perf_counter()\n",
    "            convert_json_to_npz(frame_input_path, frame_output_path, compress = compress, dtype_out = dtype_out, exclude_keys = exclude_keys)\n",
    "            if verbose == 2:\n",
    "                print(f'Frame {frame} processed in {perf_counter() - start_frame:.2f} seconds')\n",
    "        except:\n",
    "            print(f'Error processing frame {frame}. Skipping...')\n",
    "            failed_conversions.append(frame)\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(f'Archive processed in {perf_counter() - start:.2f} seconds with {len(failed_conversions)} failed conversions')\n",
    "        if len(failed_conversions) > 0:\n",
    "            print(f'Frames for which conversion to npz failed: {failed_conversions}')\n",
    "        print('\\nEstimated (from first frame) size reduction of archive: ')\n",
    "\n",
    "        frame_input_path = os.path.join(archive_path, f'frame{frame_list[0]}.json')\n",
    "        frame_output_path = os.path.join(output_folder, f'frame{frame_list[0]}.npz')\n",
    "        input_size, output_size, ratio = estimate_size_reduction(frame_input_path, frame_output_path, Nframes = len(frame_list))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\ns512_205',\n",
       " 'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\ns512_205\\\\frame2100000.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_path = 'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data'\n",
    "out_dir = os.path.join(main_path, 'compressed_data')\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "data_dirs = os.listdir(main_path)\n",
    "data_dirs = [os.path.join(main_path, d) for d in data_dirs]\n",
    "#ar = mp.archive.loadarchive(data_dirs[0])\n",
    "N = 7\n",
    "dir = data_dirs[N]\n",
    "\n",
    "outfiles_paths = []\n",
    "for file in os.listdir(dir):\n",
    "    if not file.endswith('.json'):\n",
    "        outfiles_paths.append(os.path.join(dir, file))\n",
    "\n",
    "for file in outfiles_paths:\n",
    "    os.remove(file)\n",
    "\n",
    "file_path = os.path.join(dir, os.listdir(dir)[1])\n",
    "dir, file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive processed in 16.53 seconds with 0 failed conversions\n",
      "\n",
      "Estimated size reduction for entire archive (8 frames):\n",
      "Uncompressed archive size: 313.27 MB\n",
      "Compressed archive size: 64.49 MB\n",
      "Compression ratio: 4.86x\n",
      "\n",
      "Time to open json frame: 1.15 seconds\n",
      "Time to open npz frame: 0.01 seconds\n",
      "Speedup: 135.00x\n"
     ]
    }
   ],
   "source": [
    "ca = CompressArchive(dir, dtype_out = 'float32', delete_archive_if_successful = False)\n",
    "conversion_kwargs = {'compress': True, 'exclude_keys': ['ff'], 'calc_velocities': True, 'overwrite_existing_npz_files': True, 'verbose': 1}\n",
    "ca.convert_archive_to_npz(**conversion_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\ns512_205'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = mp.archive.loadarchive(ca.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca.num_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar2 = mp.archive.loadarchive(ca.archive_dir)\n",
    "defect_list2 = get_defect_list(ar2, ar2.LX, ar2.LY, Nframes=ca.num_frames, archive_path=ca.archive_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "defect_list = get_defect_list(ar, ar.LX, ar.LY, Nframes=ca.num_frames, archive_path=ca.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'charge': 0.5, 'pos': [31.5, 188.5]},\n",
       " {'charge': 0.5, 'pos': [31.5, 308.5]},\n",
       " {'charge': 0.5, 'pos': [43.5, 265.5]},\n",
       " {'charge': -0.5, 'pos': [79.5, 435.5]},\n",
       " {'charge': -0.5, 'pos': [181.5, 231.5]},\n",
       " {'charge': 0.5, 'pos': [187.5, 402.5]},\n",
       " {'charge': 0.5, 'pos': [201.5, 75.5]},\n",
       " {'charge': 0.5, 'pos': [299.5, 404.5]},\n",
       " {'charge': -0.5, 'pos': [337.5, 410.5]},\n",
       " {'charge': -0.5, 'pos': [408.5, 16.5]},\n",
       " {'charge': -0.5, 'pos': [420.5, 235.5]},\n",
       " {'charge': -0.5, 'pos': [464.5, 396.5]}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defect_list2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'charge': 0.5, 'pos': [31.5, 188.5]},\n",
       " {'charge': 0.5, 'pos': [31.5, 308.5]},\n",
       " {'charge': 0.5, 'pos': [43.5, 265.5]},\n",
       " {'charge': -0.5, 'pos': [79.5, 435.5]},\n",
       " {'charge': -0.5, 'pos': [181.5, 231.5]},\n",
       " {'charge': 0.5, 'pos': [187.5, 402.5]},\n",
       " {'charge': 0.5, 'pos': [201.5, 75.5]},\n",
       " {'charge': 0.5, 'pos': [299.5, 404.5]},\n",
       " {'charge': -0.5, 'pos': [337.5, 410.5]},\n",
       " {'charge': -0.5, 'pos': [408.5, 16.5]},\n",
       " {'charge': -0.5, 'pos': [420.5, 235.5]},\n",
       " {'charge': -0.5, 'pos': [464.5, 396.5]}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defect_list[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_dict = {key: npz_frame[key] for key in npz_frame.files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame.__dict__.update(arr_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QQxx (1048576,)\n",
      "QQyx (1048576,)\n",
      "FFx (1048576,)\n",
      "FFy (1048576,)\n",
      "dE_kin (1048576,)\n",
      "dPE_LC (1048576,)\n",
      "density (1048576,)\n",
      "vx (1048576,)\n",
      "vy (1048576,)\n"
     ]
    }
   ],
   "source": [
    "frame_list = ca.frame_list\n",
    "npz_frame = np.load(os.path.join(ca.output_dir, f'frame{frame_list[0]}.npz'), allow_pickle=True)\n",
    "npz_frame.files\n",
    "\n",
    "for key in npz_frame.files:\n",
    "    print(key, npz_frame[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to open array and get defects using current method:  6.95262305\n"
     ]
    }
   ],
   "source": [
    "# Time to open array and get defects using current method\n",
    "t1 = perf_counter()\n",
    "ar = mp.archive.loadarchive(dir)\n",
    "frame = ar._read_frame(0)\n",
    "\n",
    "Qxx_dat = frame.QQxx.reshape(ar.LX, ar.LY)\n",
    "Qyx_dat = frame.QQyx.reshape(ar.LX, ar.LY)\n",
    "#defect_list = get_defect_list(ar, ar.LX, ar.LY, Nframes=2, archive_path=dir)\n",
    "t2 = perf_counter()\n",
    "print('Time to open array and get defects using current method: ', (t2-t1)/(len(os.listdir(dir)) -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to open array using load json:  6.14445180000007\n"
     ]
    }
   ],
   "source": [
    "# Time to open array using load json\n",
    "t1 = perf_counter()\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "arr_dict = unpack_arrays(data)\n",
    "arr_dict32 = unpack_arrays(data, dtype_out='float32')\n",
    "t2 = perf_counter()\n",
    "print('Time to open array using load json: ', t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to convert and save to numpy array, excluding ff:  7.462377200000105\n",
      "Input file size: 155.5849323272705 MB\n",
      "Output file size: 22.298182487487793 MB\n",
      "Compression ratio: 6.98x\n",
      "Time to open npz file and get defects, excluding ff:  1.4298758999998427\n"
     ]
    }
   ],
   "source": [
    "# Time to convert and save to numpy array\n",
    "t1 = perf_counter()\n",
    "npz_file_res = os.path.join(dir, os.path.join(out_dir,'data.npz'))\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "arr_dict = unpack_arrays(data, dtype_out = 'float32')\n",
    "del arr_dict['ff']\n",
    "np.savez_compressed(npz_file_res, **arr_dict)\n",
    "t2 = perf_counter()\n",
    "\n",
    "print('Time to convert and save to numpy array, excluding ff: ', t2-t1)\n",
    "print_size(file_path, npz_file_res)\n",
    "\n",
    "# Time to open npz file and unpack arrays\n",
    "t1 = perf_counter()\n",
    "npz = np.load(npz_file_res, allow_pickle=True)\n",
    "LX = ar.LX\n",
    "\n",
    "Qxx_dat = npz['QQxx'].reshape(LX, LX)\n",
    "Qyx_dat = npz['QQyx'].reshape(LX, LX)\n",
    "# Get defects\n",
    "defects = mp.nematic.nematicPy.get_defects(Qxx_dat, Qyx_dat, LX, LX)\n",
    "\n",
    "\n",
    "t2 = perf_counter()\n",
    "print('Time to open npz file and get defects, excluding ff: ', t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to convert and save to numpy array:  8.52731899999992\n",
      "Input file size: 155.5849323272705 MB\n",
      "Output file size: 50.06804656982422 MB\n",
      "Compression ratio: 3.11x\n",
      "Time to open npz file and get defects:  1.2702217999999448\n"
     ]
    }
   ],
   "source": [
    "# Time to convert and save to numpy array\n",
    "t1 = perf_counter()\n",
    "npz_file = os.path.join(dir, os.path.join(out_dir,'data.npz'))\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "arr_dict = unpack_arrays(data, dtype_out = 'float32')\n",
    "np.savez_compressed(npz_file, **arr_dict)\n",
    "t2 = perf_counter()\n",
    "\n",
    "print('Time to convert and save to numpy array: ', t2-t1)\n",
    "print_size(file_path, npz_file)\n",
    "\n",
    "# Time to open npz file and unpack arrays\n",
    "t1 = perf_counter()\n",
    "npz = np.load(npz_file, allow_pickle=True)\n",
    "LX = ar.LX\n",
    "\n",
    "Qxx_dat = npz['QQxx'].reshape(LX, LX)\n",
    "Qyx_dat = npz['QQyx'].reshape(LX, LX)\n",
    "# Get defects\n",
    "defects = mp.nematic.nematicPy.get_defects(Qxx_dat, Qyx_dat, LX, LX)\n",
    "\n",
    "\n",
    "t2 = perf_counter()\n",
    "print('Time to open npz file and get defects: ', t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to compress json file using lz4:  0.7508942000000047\n",
      "Input file size: 155.5849323272705 MB\n",
      "Output file size: 89.09958457946777 MB\n",
      "Compression ratio: 1.75x\n",
      "Time to compress json file using lz4:  4.974672699999985\n"
     ]
    }
   ],
   "source": [
    "# Time to compress json file using lz4\n",
    "t1 = perf_counter()\n",
    "output_file = file_path + '.lz4'\n",
    "compress_file(file_path, output_file)\n",
    "t2 = perf_counter()\n",
    "print('Time to compress json file using lz4: ', t2-t1)\n",
    "print_size(file_path, output_file)\n",
    "\n",
    "# Time to read lz4 file, decompress and load arrays\n",
    "t1 = perf_counter()\n",
    "json_dict = decompress_and_convert(output_file, out_format='json')\n",
    "arr_dict = unpack_arrays(json_dict)\n",
    "t2 = perf_counter()\n",
    "print('Time to compress json file using lz4: ', t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to compress npz file using lz4:  0.14694500000001653\n",
      "Input file size: 50.06804656982422 MB\n",
      "Output file size: 50.0709924697876 MB\n",
      "Compression ratio: 1.00x\n",
      "Time to read npz file using lz4:  0.05856810000000223\n"
     ]
    }
   ],
   "source": [
    "# Time to compress npz file using lz4\n",
    "t1 = perf_counter()\n",
    "output_file = npz_file + '.lz4'\n",
    "compress_file(npz_file, output_file)\n",
    "t2 = perf_counter()\n",
    "print('Time to compress npz file using lz4: ', t2-t1)\n",
    "print_size(npz_file, output_file)\n",
    "\n",
    "# Time to read lz4 file, decompress and load arrays\n",
    "t1 = perf_counter()\n",
    "arr_dict = decompress_and_convert(output_file, out_format = 'npz')\n",
    "t2 = perf_counter()\n",
    "print('Time to read npz file using lz4: ', t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.922946"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = arr_dict['QQxx']\n",
    "Q[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IDEAS\n",
    "# save each array as binary with lz4 and read directly from binary file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
