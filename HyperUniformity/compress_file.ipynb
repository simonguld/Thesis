{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import lz4.frame\n",
    "from time import perf_counter\n",
    "from multiprocessing import Pool\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from CompressArchive import CompressArchive\n",
    "from utils import *\n",
    "#import massPy as mp\n",
    "\n",
    "import massPy_dev.massPy as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\ns1024_06_npz',\n",
       " 'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\ns1024_06_npz\\\\frame50000.npz')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_path = 'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data'\n",
    "out_dir = os.path.join(main_path, 'compressed_data')\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "data_dirs = os.listdir(main_path)\n",
    "data_dirs = [os.path.join(main_path, d) for d in data_dirs]\n",
    "N = 2\n",
    "dir = data_dirs[N]\n",
    "\n",
    "outfiles_paths = []\n",
    "for file in os.listdir(dir):\n",
    "    if not file.endswith('.json'):\n",
    "        outfiles_paths.append(os.path.join(dir, file))\n",
    "\n",
    "file_path = os.path.join(dir, os.listdir(dir)[0])\n",
    "dir, file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(os.path.join(dir, 'frame*'))\n",
    "#for file in glob(os.path.join(dir, 'frame*')):\n",
    " #   print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = mp.archive.loadarchive(dir)\n",
    "frame = ar._read_frame(1)\n",
    "defects = get_defect_list(ar, Nframes=2)\n",
    "#def_arr = get_defect_arr_from_frame(defects[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\qzk1k30.05_K30.05_qkbt0_z0.022_xi1_LX256_counter0'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.dirname(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive processed in 65.35 seconds with 0 failed conversions\n",
      "\n",
      "Estimated size reduction for entire archive (181 frames):\n",
      "Uncompressed archive size: 1191.86 MB\n",
      "Compressed archive size: 199.42 MB\n",
      "Compression ratio: 5.98x\n",
      "\n",
      "Time to open json frame: 0.18 seconds\n",
      "Time to open npz frame: 0.01 seconds\n",
      "Speedup: 17.05x\n",
      "Archive C:\\Users\\Simon Andersen\\Dokumenter\\Uni\\Speciale\\Hyperuniformity\\nematic_data\\qzk1k30.05_K30.05_qkbt0_z0.1_xi1_LX256_counter0 deleted\n"
     ]
    }
   ],
   "source": [
    "ca = CompressArchive(dir, dtype_out = 'float32', delete_archive_if_successful = True)\n",
    "conversion_kwargs = {'compress': True, 'exclude_keys': ['ff'], 'calc_velocities': True, 'overwrite_existing_npz_files': True, 'verbose': 1}\n",
    "ca.convert_archive_to_npz(**conversion_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_frames(archive_path):\n",
    "\n",
    "    ar = mp.archive.loadarchive(archive_path)\n",
    "\n",
    "    dir_list = os.listdir(archive_path)\n",
    "    frame_list = []\n",
    "\n",
    "    for item in dir_list:\n",
    "        if item.startswith(\"frame\"):\n",
    "            frame_num = int(item.split('.')[0].split('frame')[-1])\n",
    "            frame_list.append(frame_num)\n",
    "\n",
    "    if len(frame_list) == ar.num_frames:\n",
    "        return np.arange(ar.nstart, ar.nsteps + 1, ar.ninfo)\n",
    "    else:\n",
    "        frame_list.sort()\n",
    "        return frame_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_npz_folder(archive_path, output_folder = None, check_for_missing_frames = False, compress = True, \\\n",
    "                      dtype_out= 'float32', exclude_keys=[], verbose = 1):\n",
    "    \"\"\"\n",
    "    verbose = 0: no output\n",
    "    verbose = 1: print time to process entire archive\n",
    "    verbose = 2: print time to process each frame\n",
    "    \"\"\"\n",
    "    # Create the output folder if it does not exist\n",
    "\n",
    "    output_folder = archive_path + '_npz' if output_folder is None else output_folder\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # copy parameters.json to output folder\n",
    "    parameters_path = os.path.join(archive_path, 'parameters.json')\n",
    "    shutil.copy(parameters_path, output_folder)\n",
    "\n",
    "    # Load the archive and get the list of frames\n",
    "    ar = mp.archive.loadarchive(archive_path)\n",
    "    frame_list = find_missing_frames(archive_path) if check_for_missing_frames else np.arange(ar.nstart, ar.nsteps + 1, ar.ninfo)\n",
    "\n",
    "    # initialize failed conversions list\n",
    "    failed_conversions = []\n",
    "\n",
    "    if verbose > 0:\n",
    "        start = time.perf_counter()\n",
    "\n",
    "    for i, frame in enumerate(frame_list):\n",
    "        frame_input_path = os.path.join(archive_path, f'frame{frame}.json')\n",
    "        frame_output_path = os.path.join(output_folder, f'frame{frame}.npz')\n",
    "        try:\n",
    "            if verbose == 2:\n",
    "                start_frame = time.perf_counter()\n",
    "            convert_json_to_npz(frame_input_path, frame_output_path, compress = compress, dtype_out = dtype_out, exclude_keys = exclude_keys)\n",
    "            if verbose == 2:\n",
    "                print(f'Frame {frame} processed in {time.perf_counter() - start_frame:.2f} seconds')\n",
    "        except:\n",
    "            print(f'Error processing frame {frame}. Skipping...')\n",
    "            failed_conversions.append(frame)\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(f'Archive processed in {time.perf_counter() - start:.2f} seconds with {len(failed_conversions)} failed conversions')\n",
    "        if len(failed_conversions) > 0:\n",
    "            print(f'Frames for which conversion to npz failed: {failed_conversions}')\n",
    "        print('\\nEstimated (from first frame) size reduction of archive: ')\n",
    "\n",
    "        frame_input_path = os.path.join(archive_path, f'frame{frame_list[0]}.json')\n",
    "        frame_output_path = os.path.join(output_folder, f'frame{frame_list[0]}.npz')\n",
    "        input_size, output_size, ratio = estimate_size_reduction(frame_input_path, frame_output_path, Nframes = len(frame_list))\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive processed in 17.18 seconds with 0 failed conversions\n",
      "\n",
      "Estimated (from first frame) size reduction of archive: \n",
      "Uncompressed archive size: 311.12 MB\n",
      "Compressed archive size: 100.17 MB\n",
      "Compression ratio: 3.11x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_npz_folder(dir, compress = True, check_for_missing_frames=True, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = mp.archive.loadarchive(ca.output_dir)\n",
    "frame = ar._read_frame(0)\n",
    "defects = get_defect_list(ar, Nframes=1)\n",
    "def_arr = get_defect_arr_from_frame(defects[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to open array and get defects using current method:  6.95262305\n"
     ]
    }
   ],
   "source": [
    "# Time to open array and get defects using current method\n",
    "t1 = perf_counter()\n",
    "ar = mp.archive.loadarchive(dir)\n",
    "frame = ar._read_frame(0)\n",
    "\n",
    "Qxx_dat = frame.QQxx.reshape(ar.LX, ar.LY)\n",
    "Qyx_dat = frame.QQyx.reshape(ar.LX, ar.LY)\n",
    "#defect_list = get_defect_list(ar, ar.LX, ar.LY, Nframes=2, archive_path=dir)\n",
    "t2 = perf_counter()\n",
    "print('Time to open array and get defects using current method: ', (t2-t1)/(len(os.listdir(dir)) -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to open array using load json:  6.14445180000007\n"
     ]
    }
   ],
   "source": [
    "# Time to open array using load json\n",
    "t1 = perf_counter()\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "arr_dict = unpack_arrays(data)\n",
    "arr_dict32 = unpack_arrays(data, dtype_out='float32')\n",
    "t2 = perf_counter()\n",
    "print('Time to open array using load json: ', t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to convert and save to numpy array, excluding ff:  7.462377200000105\n",
      "Input file size: 155.5849323272705 MB\n",
      "Output file size: 22.298182487487793 MB\n",
      "Compression ratio: 6.98x\n",
      "Time to open npz file and get defects, excluding ff:  1.4298758999998427\n"
     ]
    }
   ],
   "source": [
    "# Time to convert and save to numpy array\n",
    "t1 = perf_counter()\n",
    "npz_file_res = os.path.join(dir, os.path.join(out_dir,'data.npz'))\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "arr_dict = unpack_arrays(data, dtype_out = 'float32')\n",
    "del arr_dict['ff']\n",
    "np.savez_compressed(npz_file_res, **arr_dict)\n",
    "t2 = perf_counter()\n",
    "\n",
    "print('Time to convert and save to numpy array, excluding ff: ', t2-t1)\n",
    "print_size(file_path, npz_file_res)\n",
    "\n",
    "# Time to open npz file and unpack arrays\n",
    "t1 = perf_counter()\n",
    "npz = np.load(npz_file_res, allow_pickle=True)\n",
    "LX = ar.LX\n",
    "\n",
    "Qxx_dat = npz['QQxx'].reshape(LX, LX)\n",
    "Qyx_dat = npz['QQyx'].reshape(LX, LX)\n",
    "# Get defects\n",
    "defects = mp.nematic.nematicPy.get_defects(Qxx_dat, Qyx_dat, LX, LX)\n",
    "\n",
    "\n",
    "t2 = perf_counter()\n",
    "print('Time to open npz file and get defects, excluding ff: ', t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to convert and save to numpy array:  8.52731899999992\n",
      "Input file size: 155.5849323272705 MB\n",
      "Output file size: 50.06804656982422 MB\n",
      "Compression ratio: 3.11x\n",
      "Time to open npz file and get defects:  1.2702217999999448\n"
     ]
    }
   ],
   "source": [
    "# Time to convert and save to numpy array\n",
    "t1 = perf_counter()\n",
    "npz_file = os.path.join(dir, os.path.join(out_dir,'data.npz'))\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "arr_dict = unpack_arrays(data, dtype_out = 'float32')\n",
    "np.savez_compressed(npz_file, **arr_dict)\n",
    "t2 = perf_counter()\n",
    "\n",
    "print('Time to convert and save to numpy array: ', t2-t1)\n",
    "print_size(file_path, npz_file)\n",
    "\n",
    "# Time to open npz file and unpack arrays\n",
    "t1 = perf_counter()\n",
    "npz = np.load(npz_file, allow_pickle=True)\n",
    "LX = ar.LX\n",
    "\n",
    "Qxx_dat = npz['QQxx'].reshape(LX, LX)\n",
    "Qyx_dat = npz['QQyx'].reshape(LX, LX)\n",
    "# Get defects\n",
    "defects = mp.nematic.nematicPy.get_defects(Qxx_dat, Qyx_dat, LX, LX)\n",
    "\n",
    "\n",
    "t2 = perf_counter()\n",
    "print('Time to open npz file and get defects: ', t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to compress json file using lz4:  0.7508942000000047\n",
      "Input file size: 155.5849323272705 MB\n",
      "Output file size: 89.09958457946777 MB\n",
      "Compression ratio: 1.75x\n",
      "Time to compress json file using lz4:  4.974672699999985\n"
     ]
    }
   ],
   "source": [
    "# Time to compress json file using lz4\n",
    "t1 = perf_counter()\n",
    "output_file = file_path + '.lz4'\n",
    "compress_file(file_path, output_file)\n",
    "t2 = perf_counter()\n",
    "print('Time to compress json file using lz4: ', t2-t1)\n",
    "print_size(file_path, output_file)\n",
    "\n",
    "# Time to read lz4 file, decompress and load arrays\n",
    "t1 = perf_counter()\n",
    "json_dict = decompress_and_convert(output_file, out_format='json')\n",
    "arr_dict = unpack_arrays(json_dict)\n",
    "t2 = perf_counter()\n",
    "print('Time to compress json file using lz4: ', t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to compress npz file using lz4:  0.14694500000001653\n",
      "Input file size: 50.06804656982422 MB\n",
      "Output file size: 50.0709924697876 MB\n",
      "Compression ratio: 1.00x\n",
      "Time to read npz file using lz4:  0.05856810000000223\n"
     ]
    }
   ],
   "source": [
    "# Time to compress npz file using lz4\n",
    "t1 = perf_counter()\n",
    "output_file = npz_file + '.lz4'\n",
    "compress_file(npz_file, output_file)\n",
    "t2 = perf_counter()\n",
    "print('Time to compress npz file using lz4: ', t2-t1)\n",
    "print_size(npz_file, output_file)\n",
    "\n",
    "# Time to read lz4 file, decompress and load arrays\n",
    "t1 = perf_counter()\n",
    "arr_dict = decompress_and_convert(output_file, out_format = 'npz')\n",
    "t2 = perf_counter()\n",
    "print('Time to read npz file using lz4: ', t2-t1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
