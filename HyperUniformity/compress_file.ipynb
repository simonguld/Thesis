{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "import lz4.frame\n",
    "from time import perf_counter\n",
    "from multiprocessing import Pool\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from CompressArchive import CompressArchive\n",
    "from utils import *\n",
    "import massPy as mp\n",
    "\n",
    "\n",
    "#import massPy_dev.massPy as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\qzk1k30.05_K30.05_qkbt0_z0.022_xi1_LX256_counter0',\n",
       " 'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\qzk1k30.05_K30.05_qkbt0_z0.022_xi1_LX256_counter0\\\\frame10000.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_path = 'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data'\n",
    "out_dir = os.path.join(main_path, 'compressed_data')\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "data_dirs = os.listdir(main_path)\n",
    "data_dirs = [os.path.join(main_path, d) for d in data_dirs]\n",
    "N = -3\n",
    "dir = data_dirs[N]\n",
    "\n",
    "outfiles_paths = []\n",
    "for file in os.listdir(dir):\n",
    "    if not file.endswith('.json'):\n",
    "        outfiles_paths.append(os.path.join(dir, file))\n",
    "\n",
    "for file in outfiles_paths:\n",
    "    os.remove(file)\n",
    "\n",
    "file_path = os.path.join(dir, os.listdir(dir)[0])\n",
    "dir, file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(os.path.join(dir, 'frame*'))\n",
    "#for file in glob(os.path.join(dir, 'frame*')):\n",
    " #   print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\qzk1k30.05_K30.05_qkbt0_z0.022_xi1_LX256_counter0'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.dirname(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent = os.path.join(files[0], os.pardir) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\qzk1k30.05_K30.05_qkbt0_z0.022_xi1_LX256_counter0\\\\frame10000.json\\\\..'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\compressed_data',\n",
       " 'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\ns1024_06',\n",
       " 'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\ns1024_06_npz',\n",
       " 'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\ns512_06',\n",
       " 'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\ns512_19n16',\n",
       " 'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\ns512_19n16_compressed',\n",
       " 'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\ns512_19n3',\n",
       " 'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\ns512_205',\n",
       " 'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\ns512_205_compressed',\n",
       " 'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\ns512_21',\n",
       " 'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\ns512_22',\n",
       " 'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\ns512_28',\n",
       " 'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\ns512_a12n0',\n",
       " 'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\ns512_a16n0',\n",
       " 'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\ns512_a18n0',\n",
       " 'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\qzk1k30.05_K30.05_qkbt0_z0.022_xi1_LX256_counter0',\n",
       " 'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\qzk1k30.05_K30.05_qkbt0_z0.022_xi1_LX256_counter0_npz',\n",
       " 'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\qzk1k30.05_K30.05_qkbt0_z0.1_xi1_LX256_counter0_compressed']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive processed in 65.35 seconds with 0 failed conversions\n",
      "\n",
      "Estimated size reduction for entire archive (181 frames):\n",
      "Uncompressed archive size: 1191.86 MB\n",
      "Compressed archive size: 199.42 MB\n",
      "Compression ratio: 5.98x\n",
      "\n",
      "Time to open json frame: 0.18 seconds\n",
      "Time to open npz frame: 0.01 seconds\n",
      "Speedup: 17.05x\n",
      "Archive C:\\Users\\Simon Andersen\\Dokumenter\\Uni\\Speciale\\Hyperuniformity\\nematic_data\\qzk1k30.05_K30.05_qkbt0_z0.1_xi1_LX256_counter0 deleted\n"
     ]
    }
   ],
   "source": [
    "ca = CompressArchive(dir, dtype_out = 'float32', delete_archive_if_successful = True)\n",
    "conversion_kwargs = {'compress': True, 'exclude_keys': ['ff'], 'calc_velocities': True, 'overwrite_existing_npz_files': True, 'verbose': 1}\n",
    "ca.convert_archive_to_npz(**conversion_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_frames(archive_path):\n",
    "\n",
    "    ar = mp.archive.loadarchive(archive_path)\n",
    "\n",
    "    dir_list = os.listdir(archive_path)\n",
    "    frame_list = []\n",
    "\n",
    "    for item in dir_list:\n",
    "        if item.startswith(\"frame\"):\n",
    "            frame_num = int(item.split('.')[0].split('frame')[-1])\n",
    "            frame_list.append(frame_num)\n",
    "\n",
    "    if len(frame_list) == ar.num_frames:\n",
    "        return np.arange(ar.nstart, ar.nsteps + 1, ar.ninfo)\n",
    "    else:\n",
    "        frame_list.sort()\n",
    "        return frame_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\ns1024_06'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_npz_folder(archive_path, output_folder = None, check_for_missing_frames = False, compress = True, \\\n",
    "                      dtype_out= 'float32', exclude_keys=[], verbose = 1):\n",
    "    \"\"\"\n",
    "    verbose = 0: no output\n",
    "    verbose = 1: print time to process entire archive\n",
    "    verbose = 2: print time to process each frame\n",
    "    \"\"\"\n",
    "    # Create the output folder if it does not exist\n",
    "\n",
    "    output_folder = archive_path + '_npz' if output_folder is None else output_folder\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # copy parameters.json to output folder\n",
    "    parameters_path = os.path.join(archive_path, 'parameters.json')\n",
    "    shutil.copy(parameters_path, output_folder)\n",
    "\n",
    "    # Load the archive and get the list of frames\n",
    "    ar = mp.archive.loadarchive(archive_path)\n",
    "    frame_list = find_missing_frames(archive_path) if check_for_missing_frames else np.arange(ar.nstart, ar.nsteps + 1, ar.ninfo)\n",
    "\n",
    "    # initialize failed conversions list\n",
    "    failed_conversions = []\n",
    "\n",
    "    if verbose > 0:\n",
    "        start = time.perf_counter()\n",
    "\n",
    "    for i, frame in enumerate(frame_list):\n",
    "        frame_input_path = os.path.join(archive_path, f'frame{frame}.json')\n",
    "        frame_output_path = os.path.join(output_folder, f'frame{frame}.npz')\n",
    "        try:\n",
    "            if verbose == 2:\n",
    "                start_frame = time.perf_counter()\n",
    "            convert_json_to_npz(frame_input_path, frame_output_path, compress = compress, dtype_out = dtype_out, exclude_keys = exclude_keys)\n",
    "            if verbose == 2:\n",
    "                print(f'Frame {frame} processed in {time.perf_counter() - start_frame:.2f} seconds')\n",
    "        except:\n",
    "            print(f'Error processing frame {frame}. Skipping...')\n",
    "            failed_conversions.append(frame)\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(f'Archive processed in {time.perf_counter() - start:.2f} seconds with {len(failed_conversions)} failed conversions')\n",
    "        if len(failed_conversions) > 0:\n",
    "            print(f'Frames for which conversion to npz failed: {failed_conversions}')\n",
    "        print('\\nEstimated (from first frame) size reduction of archive: ')\n",
    "\n",
    "        frame_input_path = os.path.join(archive_path, f'frame{frame_list[0]}.json')\n",
    "        frame_output_path = os.path.join(output_folder, f'frame{frame_list[0]}.npz')\n",
    "        input_size, output_size, ratio = estimate_size_reduction(frame_input_path, frame_output_path, Nframes = len(frame_list))\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive processed in 17.18 seconds with 0 failed conversions\n",
      "\n",
      "Estimated (from first frame) size reduction of archive: \n",
      "Uncompressed archive size: 311.12 MB\n",
      "Compressed archive size: 100.17 MB\n",
      "Compression ratio: 3.11x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "create_npz_folder(dir, compress = True, check_for_missing_frames=True, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = ['wet_z_phasetwist05/']\n",
    "\n",
    "for path in paths:\n",
    "    names = glob.glob(f'{path}/*/*')\n",
    "\n",
    "nnames = len(names)\n",
    "print(nnames)\n",
    "#sys.exit()\n",
    "ntasks = min(nnames, os.cpu_count())\n",
    "csize = 10\n",
    "\n",
    "with Pool(ntasks) as p:\n",
    "        p.map(zip_file, names, chunksize=csize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_json_to_npz(frame_number, archive_path, output_folder, compress = True, dtype_out = 'float64', exclude_keys=[]):\n",
    "\n",
    "    frame_input_path = os.path.join(archive_path, f'frame{frame_number}.json')\n",
    "    frame_output_path = os.path.join(output_folder, f'frame{frame_number}.npz')\n",
    "\n",
    "    with open(frame_input_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    keys = list(data['data'].keys())\n",
    "    arr_dict = {}\n",
    "    arr_dict = {key: np.array(data['data'][key]['value'],dtype=dtype_out) for key in keys if key not in exclude_keys}\n",
    "\n",
    "\n",
    "    if compress:\n",
    "        np.savez_compressed(frame_output_path, **arr_dict)\n",
    "    else:\n",
    "        np.savez(frame_output_path, **arr_dict)\n",
    "    print(f'Frame {frame_number} processed')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_npz_folder2(archive_path, output_folder = None, check_for_missing_frames = False, compress = True, \\\n",
    "                      dtype_out= 'float32', exclude_keys=[], verbose = 1):\n",
    "    \"\"\"\n",
    "    verbose = 0: no output\n",
    "    verbose = 1: print time to process entire archive\n",
    "    verbose = 2: print time to process each frame\n",
    "    \"\"\"\n",
    "    # Create the output folder if it does not exist\n",
    "\n",
    "    output_folder = archive_path + '_npz' if output_folder is None else output_folder\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # copy parameters.json to output folder\n",
    "    parameters_path = os.path.join(archive_path, 'parameters.json')\n",
    "    shutil.copy(parameters_path, output_folder)\n",
    "\n",
    "    # Load the archive and get the list of frames\n",
    "    ar = mp.archive.loadarchive(archive_path)\n",
    "    frame_list = find_missing_frames(archive_path) if check_for_missing_frames else np.arange(ar.nstart, ar.nsteps + 1, ar.ninfo)\n",
    "\n",
    "    # initialize failed conversions list\n",
    "    failed_conversions = []\n",
    "\n",
    "    if verbose > 0:\n",
    "        start = time.perf_counter()\n",
    "\n",
    "    \n",
    "  \n",
    "    ntasks = min(len(frame_list), os.cpu_count())\n",
    "    csize = 10\n",
    "\n",
    "    conversion_func = lambda frame_number: convert_json_to_npz(frame_number, \\\n",
    "                                         archive_path=archive_path, output_folder=output_folder, compress = compress, dtype_out = dtype_out, exclude_keys = exclude_keys)\n",
    "\n",
    "    print(frame_list)\n",
    "    print(ntasks)\n",
    "    print(csize)\n",
    "    with Pool(ntasks) as p:\n",
    "            p.map_async(conversion_func, frame_list, chunksize=csize)\n",
    "\n",
    "    if 0:\n",
    "        for i, frame in enumerate(frame_list):\n",
    "            frame_input_path = os.path.join(archive_path, f'frame{frame}.json')\n",
    "            frame_output_path = os.path.join(output_folder, f'frame{frame}.npz')\n",
    "            try:\n",
    "                if verbose == 2:\n",
    "                    start_frame = time.perf_counter()\n",
    "                convert_json_to_npz(frame_input_path, frame_output_path, compress = compress, dtype_out = dtype_out, exclude_keys = exclude_keys)\n",
    "                if verbose == 2:\n",
    "                    print(f'Frame {frame} processed in {time.perf_counter() - start_frame:.2f} seconds')\n",
    "            except:\n",
    "                print(f'Error processing frame {frame}. Skipping...')\n",
    "                failed_conversions.append(frame)\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(f'Archive processed in {time.perf_counter() - start:.2f} seconds')\n",
    "        if len(failed_conversions) > 0:\n",
    "            print(f'Frames for which conversion to npz failed: {failed_conversions}')\n",
    "        print('\\nEstimated (from first frame) size reduction of archive: ')\n",
    "\n",
    "        frame_input_path = os.path.join(archive_path, f'frame{frame_list[0]}.json')\n",
    "        frame_output_path = os.path.join(output_folder, f'frame{frame_list[0]}.npz')\n",
    "        input_size, output_size, ratio = estimate_size_reduction(frame_input_path, frame_output_path, Nframes = len(frame_list))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(np.arange(0, 10, 1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame 10000 processed\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <function <lambda> at 0x00000292B6E8AE50>: attribute lookup <lambda> on __main__ failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15216\\1173330424.py\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mntasks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconversion_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\Simon Andersen\\miniconda3\\envs\\sf\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         '''\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Simon Andersen\\miniconda3\\envs\\sf\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    769\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Simon Andersen\\miniconda3\\envs\\sf\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36m_handle_tasks\u001b[1;34m(taskqueue, put, outqueue, pool, cache)\u001b[0m\n\u001b[0;32m    535\u001b[0m                         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    536\u001b[0m                     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 537\u001b[1;33m                         \u001b[0mput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    538\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m                         \u001b[0mjob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Simon Andersen\\miniconda3\\envs\\sf\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_writable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_send_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ForkingPickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrecv_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Simon Andersen\\miniconda3\\envs\\sf\\lib\\multiprocessing\\reduction.py\u001b[0m in \u001b[0;36mdumps\u001b[1;34m(cls, obj, protocol)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPicklingError\u001b[0m: Can't pickle <function <lambda> at 0x00000292B6E8AE50>: attribute lookup <lambda> on __main__ failed"
     ]
    }
   ],
   "source": [
    "frame_list= list(np.arange(10000, 14000, 500))\n",
    "\n",
    "ntasks = min(len(frame_list), 2)\n",
    "csize = 10\n",
    "\n",
    "archive_path = dir  \n",
    "output_folder = dir + '_npz'\n",
    "compress = True\n",
    "dtype_out= 'float32'\n",
    "exclude_keys=[]\n",
    "conversion_func = lambda frame_number: convert_json_to_npz(frame_number, \\\n",
    "                                      archive_path=archive_path, output_folder=output_folder, compress = compress, dtype_out = dtype_out, exclude_keys = exclude_keys)\n",
    "\n",
    "\n",
    "conversion_func(frame_list[0])\n",
    "\n",
    "with Pool(ntasks) as p:\n",
    "        p.map(conversion_func, frame_list, chunksize=csize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print \"[0, 1, 4,..., 81]\"\n",
    "def f(x):\n",
    "    return x*x\n",
    "with Pool(5) as pool:\n",
    "    print(pool.map(f, range(10)))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10000  10500  11000  11500  12000  12500  13000  13500  14000  14500\n",
      "  15000  15500  16000  16500  17000  17500  18000  18500  19000  19500\n",
      "  20000  20500  21000  21500  22000  22500  23000  23500  24000  24500\n",
      "  25000  25500  26000  26500  27000  27500  28000  28500  29000  29500\n",
      "  30000  30500  31000  31500  32000  32500  33000  33500  34000  34500\n",
      "  35000  35500  36000  36500  37000  37500  38000  38500  39000  39500\n",
      "  40000  40500  41000  41500  42000  42500  43000  43500  44000  44500\n",
      "  45000  45500  46000  46500  47000  47500  48000  48500  49000  49500\n",
      "  50000  50500  51000  51500  52000  52500  53000  53500  54000  54500\n",
      "  55000  55500  56000  56500  57000  57500  58000  58500  59000  59500\n",
      "  60000  60500  61000  61500  62000  62500  63000  63500  64000  64500\n",
      "  65000  65500  66000  66500  67000  67500  68000  68500  69000  69500\n",
      "  70000  70500  71000  71500  72000  72500  73000  73500  74000  74500\n",
      "  75000  75500  76000  76500  77000  77500  78000  78500  79000  79500\n",
      "  80000  80500  81000  81500  82000  82500  83000  83500  84000  84500\n",
      "  85000  85500  86000  86500  87000  87500  88000  88500  89000  89500\n",
      "  90000  90500  91000  91500  92000  92500  93000  93500  94000  94500\n",
      "  95000  95500  96000  96500  97000  97500  98000  98500  99000  99500\n",
      " 100000]\n",
      "8\n",
      "10\n",
      "Archive processed in 0.07 seconds\n",
      "\n",
      "Estimated (from first frame) size reduction of archive: \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] Den angivne fil blev ikke fundet: 'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\qzk1k30.05_K30.05_qkbt0_z0.022_xi1_LX256_counter0_npz\\\\frame10000.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15216\\1885075073.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcreate_npz_folder2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompress\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_for_missing_frames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15216\\2328751504.py\u001b[0m in \u001b[0;36mcreate_npz_folder2\u001b[1;34m(archive_path, output_folder, check_for_missing_frames, compress, dtype_out, exclude_keys, verbose)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mframe_input_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marchive_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf'frame{frame_list[0]}.json'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mframe_output_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf'frame{frame_list[0]}.npz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimate_size_reduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe_input_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe_output_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Simon Andersen\\Projects\\Projects\\Thesis\\HyperUniformity\\utils.py\u001b[0m in \u001b[0;36mestimate_size_reduction\u001b[1;34m(input_file, output_file, Nframes, verbose)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;31m# Get the size of the output file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m     \u001b[0moutput_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;31m# Calculate the compression ratio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Simon Andersen\\miniconda3\\envs\\sf\\lib\\genericpath.py\u001b[0m in \u001b[0;36mgetsize\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;34m\"\"\"Return the size of a file, reported by os.stat().\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mst_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] Den angivne fil blev ikke fundet: 'C:\\\\Users\\\\Simon Andersen\\\\Dokumenter\\\\Uni\\\\Speciale\\\\Hyperuniformity\\\\nematic_data\\\\qzk1k30.05_K30.05_qkbt0_z0.022_xi1_LX256_counter0_npz\\\\frame10000.npz'"
     ]
    }
   ],
   "source": [
    "create_npz_folder2(dir, compress = True, check_for_missing_frames=True, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = mp.archive.loadarchive(ca.output_dir)\n",
    "frame = ar._read_frame(0)\n",
    "defects = get_defect_list(ar, Nframes=1)\n",
    "def_arr = get_defect_arr_from_frame(defects[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to open array and get defects using current method:  6.95262305\n"
     ]
    }
   ],
   "source": [
    "# Time to open array and get defects using current method\n",
    "t1 = perf_counter()\n",
    "ar = mp.archive.loadarchive(dir)\n",
    "frame = ar._read_frame(0)\n",
    "\n",
    "Qxx_dat = frame.QQxx.reshape(ar.LX, ar.LY)\n",
    "Qyx_dat = frame.QQyx.reshape(ar.LX, ar.LY)\n",
    "#defect_list = get_defect_list(ar, ar.LX, ar.LY, Nframes=2, archive_path=dir)\n",
    "t2 = perf_counter()\n",
    "print('Time to open array and get defects using current method: ', (t2-t1)/(len(os.listdir(dir)) -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to open array using load json:  6.14445180000007\n"
     ]
    }
   ],
   "source": [
    "# Time to open array using load json\n",
    "t1 = perf_counter()\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "arr_dict = unpack_arrays(data)\n",
    "arr_dict32 = unpack_arrays(data, dtype_out='float32')\n",
    "t2 = perf_counter()\n",
    "print('Time to open array using load json: ', t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to convert and save to numpy array, excluding ff:  7.462377200000105\n",
      "Input file size: 155.5849323272705 MB\n",
      "Output file size: 22.298182487487793 MB\n",
      "Compression ratio: 6.98x\n",
      "Time to open npz file and get defects, excluding ff:  1.4298758999998427\n"
     ]
    }
   ],
   "source": [
    "# Time to convert and save to numpy array\n",
    "t1 = perf_counter()\n",
    "npz_file_res = os.path.join(dir, os.path.join(out_dir,'data.npz'))\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "arr_dict = unpack_arrays(data, dtype_out = 'float32')\n",
    "del arr_dict['ff']\n",
    "np.savez_compressed(npz_file_res, **arr_dict)\n",
    "t2 = perf_counter()\n",
    "\n",
    "print('Time to convert and save to numpy array, excluding ff: ', t2-t1)\n",
    "print_size(file_path, npz_file_res)\n",
    "\n",
    "# Time to open npz file and unpack arrays\n",
    "t1 = perf_counter()\n",
    "npz = np.load(npz_file_res, allow_pickle=True)\n",
    "LX = ar.LX\n",
    "\n",
    "Qxx_dat = npz['QQxx'].reshape(LX, LX)\n",
    "Qyx_dat = npz['QQyx'].reshape(LX, LX)\n",
    "# Get defects\n",
    "defects = mp.nematic.nematicPy.get_defects(Qxx_dat, Qyx_dat, LX, LX)\n",
    "\n",
    "\n",
    "t2 = perf_counter()\n",
    "print('Time to open npz file and get defects, excluding ff: ', t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to convert and save to numpy array:  8.52731899999992\n",
      "Input file size: 155.5849323272705 MB\n",
      "Output file size: 50.06804656982422 MB\n",
      "Compression ratio: 3.11x\n",
      "Time to open npz file and get defects:  1.2702217999999448\n"
     ]
    }
   ],
   "source": [
    "# Time to convert and save to numpy array\n",
    "t1 = perf_counter()\n",
    "npz_file = os.path.join(dir, os.path.join(out_dir,'data.npz'))\n",
    "with open(file_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "arr_dict = unpack_arrays(data, dtype_out = 'float32')\n",
    "np.savez_compressed(npz_file, **arr_dict)\n",
    "t2 = perf_counter()\n",
    "\n",
    "print('Time to convert and save to numpy array: ', t2-t1)\n",
    "print_size(file_path, npz_file)\n",
    "\n",
    "# Time to open npz file and unpack arrays\n",
    "t1 = perf_counter()\n",
    "npz = np.load(npz_file, allow_pickle=True)\n",
    "LX = ar.LX\n",
    "\n",
    "Qxx_dat = npz['QQxx'].reshape(LX, LX)\n",
    "Qyx_dat = npz['QQyx'].reshape(LX, LX)\n",
    "# Get defects\n",
    "defects = mp.nematic.nematicPy.get_defects(Qxx_dat, Qyx_dat, LX, LX)\n",
    "\n",
    "\n",
    "t2 = perf_counter()\n",
    "print('Time to open npz file and get defects: ', t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to compress json file using lz4:  0.7508942000000047\n",
      "Input file size: 155.5849323272705 MB\n",
      "Output file size: 89.09958457946777 MB\n",
      "Compression ratio: 1.75x\n",
      "Time to compress json file using lz4:  4.974672699999985\n"
     ]
    }
   ],
   "source": [
    "# Time to compress json file using lz4\n",
    "t1 = perf_counter()\n",
    "output_file = file_path + '.lz4'\n",
    "compress_file(file_path, output_file)\n",
    "t2 = perf_counter()\n",
    "print('Time to compress json file using lz4: ', t2-t1)\n",
    "print_size(file_path, output_file)\n",
    "\n",
    "# Time to read lz4 file, decompress and load arrays\n",
    "t1 = perf_counter()\n",
    "json_dict = decompress_and_convert(output_file, out_format='json')\n",
    "arr_dict = unpack_arrays(json_dict)\n",
    "t2 = perf_counter()\n",
    "print('Time to compress json file using lz4: ', t2-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to compress npz file using lz4:  0.14694500000001653\n",
      "Input file size: 50.06804656982422 MB\n",
      "Output file size: 50.0709924697876 MB\n",
      "Compression ratio: 1.00x\n",
      "Time to read npz file using lz4:  0.05856810000000223\n"
     ]
    }
   ],
   "source": [
    "# Time to compress npz file using lz4\n",
    "t1 = perf_counter()\n",
    "output_file = npz_file + '.lz4'\n",
    "compress_file(npz_file, output_file)\n",
    "t2 = perf_counter()\n",
    "print('Time to compress npz file using lz4: ', t2-t1)\n",
    "print_size(npz_file, output_file)\n",
    "\n",
    "# Time to read lz4 file, decompress and load arrays\n",
    "t1 = perf_counter()\n",
    "arr_dict = decompress_and_convert(output_file, out_format = 'npz')\n",
    "t2 = perf_counter()\n",
    "print('Time to read npz file using lz4: ', t2-t1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
